{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2pix.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "lpcC03kKD_qs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive for Colab Use Only"
      ]
    },
    {
      "metadata": {
        "id": "mRg4Gvu2gz0u",
        "colab_type": "code",
        "outputId": "05a0d50b-961b-4b84-a380-0f65ce75e390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bEC8UWfmELA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import the Necessary Libraries"
      ]
    },
    {
      "metadata": {
        "id": "kJpV4-Id8Jll",
        "colab_type": "code",
        "outputId": "e41138ef-ee0c-4ac1-ba99-727bd7d6325d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "from PIL import Image"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uv0kvpLCERir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Related Functions"
      ]
    },
    {
      "metadata": {
        "id": "1Bz5fCVg8LIO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_train(sizeOfBatch=1):\n",
        "  traindir = '/content/drive/My Drive/Colab Notebooks/CycleGANs/ML_Dataset_5/train/' #Enter your own directory of TRAINING images here\n",
        "  res = (256,256)\n",
        "  numBatches = int(len(glob(traindir+'*'))/sizeOfBatch)\n",
        "  for batch in range(numBatches-1):\n",
        "    currBatch = glob(traindir+'*')[sizeOfBatch*batch:sizeOfBatch*(batch+1)]\n",
        "    imgA = []\n",
        "    imgB = []\n",
        "    for i in currBatch:         #Enter the name of your first and last images here [216.jpg and 349.jpg] in this case\n",
        "      img = scipy.misc.imread(i, mode='RGB').astype(np.float)#plt.imread(traindir+str(i)+'.jpg')                #Edit the format of images if neccessary\n",
        "      width = img.shape[1]\n",
        "      a = scipy.misc.imresize(img[:,:width//2,:],res)\n",
        "      b = scipy.misc.imresize(img[:,width//2:,:],res)\n",
        "      if np.random.random() > 0.5:\n",
        "        a = np.fliplr(a)\n",
        "        b = np.fliplr(b)\n",
        "      imgA.append(a)\n",
        "      imgB.append(b)\n",
        "    imgsA = normalizeIMG(imgA)\n",
        "    imgsB = normalizeIMG(imgB)\n",
        "    yield imgsA, imgsB\n",
        "\n",
        "def load_test():\n",
        "  testdir = '/content/drive/My Drive/Colab Notebooks/CycleGANs/ML_Dataset_5/test/'  #Enter your own directory of TEST images here\n",
        "  testA = []\n",
        "  testB = []\n",
        "  res = (256,256)\n",
        "  testIMGs = np.random.choice(glob(testdir+'*'), 1)\n",
        "  for i in testIMGs:\n",
        "    img = scipy.misc.imread(i, mode='RGB').astype(np.float)\n",
        "    width = img.shape[1]\n",
        "    testA.append(scipy.misc.imresize(img[:,:width//2,:],res))\n",
        "    testB.append(scipy.misc.imresize(img[:,width//2:,:],res))\n",
        "  testsA = normalizeIMG(testA)\n",
        "  testsB = normalizeIMG(testB)\n",
        "  return testsA, testsB\n",
        "\n",
        "def normalizeIMG(img):\n",
        "  return np.array(img)/127.5 - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KGEjoo3fEgwc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Related Functions"
      ]
    },
    {
      "metadata": {
        "id": "ImeYn31k81dL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "  \n",
        "    InIMG = Input(shape=img_shape)\n",
        "    \n",
        "    down1 = Conv2D(64, kernel_size=4, strides=2, padding='same')(InIMG)\n",
        "    down1 = LeakyReLU(alpha=0.2)(down1)\n",
        "    \n",
        "    down2 = Conv2D(128, kernel_size=4, strides=2, padding='same')(down1)\n",
        "    down2 = LeakyReLU(alpha=0.2)(down2)\n",
        "    down2 = BatchNormalization(momentum=0.8)(down2)\n",
        "    \n",
        "    down3 = Conv2D(256, kernel_size=4, strides=2, padding='same')(down2)\n",
        "    down3 = LeakyReLU(alpha=0.2)(down3)\n",
        "    down3 = BatchNormalization(momentum=0.8)(down3)\n",
        "    \n",
        "    down4 = Conv2D(512, kernel_size=4, strides=2, padding='same')(down3)\n",
        "    down4 = LeakyReLU(alpha=0.2)(down4)\n",
        "    down4 = BatchNormalization(momentum=0.8)(down4)\n",
        "    \n",
        "    down5 = Conv2D(512, kernel_size=4, strides=2, padding='same')(down4)\n",
        "    down5 = LeakyReLU(alpha=0.2)(down5)\n",
        "    down5 = BatchNormalization(momentum=0.8)(down5)\n",
        "    \n",
        "    down6 = Conv2D(512, kernel_size=4, strides=2, padding='same')(down5)\n",
        "    down6 = LeakyReLU(alpha=0.2)(down6)\n",
        "    down6 = BatchNormalization(momentum=0.8)(down6)\n",
        "    \n",
        "    down7 = Conv2D(512, kernel_size=4, strides=2, padding='same')(down6)\n",
        "    down7 = LeakyReLU(alpha=0.2)(down7)\n",
        "    down7 = BatchNormalization(momentum=0.8)(down7)\n",
        "\n",
        "    \n",
        "    \n",
        "    up1 = UpSampling2D(size=2)(down7)\n",
        "    up1 = Conv2D(512, kernel_size=4, strides=1, padding='same', activation='relu')(up1)\n",
        "    up1 = BatchNormalization(momentum=0.8)(up1)\n",
        "    up1 = Concatenate()([up1, down6])\n",
        "    \n",
        "    up2 = UpSampling2D(size=2)(up1)\n",
        "    up2 = Conv2D(512, kernel_size=4, strides=1, padding='same', activation='relu')(up2)\n",
        "    up2 = BatchNormalization(momentum=0.8)(up2)\n",
        "    up2 = Concatenate()([up2, down5])\n",
        "    \n",
        "    up3 = UpSampling2D(size=2)(up2)\n",
        "    up3 = Conv2D(512, kernel_size=4, strides=1, padding='same', activation='relu')(up3)\n",
        "    up3 = BatchNormalization(momentum=0.8)(up3)\n",
        "    up3 = Concatenate()([up3, down4])\n",
        "    \n",
        "    up4 = UpSampling2D(size=2)(up3)\n",
        "    up4 = Conv2D(256, kernel_size=4, strides=1, padding='same', activation='relu')(up4)\n",
        "    up4 = BatchNormalization(momentum=0.8)(up4)\n",
        "    up4 = Concatenate()([up4, down3])\n",
        "    \n",
        "    up5 = UpSampling2D(size=2)(up4)\n",
        "    up5 = Conv2D(128, kernel_size=4, strides=1, padding='same', activation='relu')(up5)\n",
        "    up5 = BatchNormalization(momentum=0.8)(up5)\n",
        "    up5 = Concatenate()([up5, down2])\n",
        "    \n",
        "    up6 = UpSampling2D(size=2)(up5)\n",
        "    up6 = Conv2D(64, kernel_size=4, strides=1, padding='same', activation='relu')(up6)\n",
        "    up6 = BatchNormalization(momentum=0.8)(up6)\n",
        "    up6 = Concatenate()([up6, down1])\n",
        "    \n",
        "    up7 = UpSampling2D(size=2)(up6)\n",
        "    OutIMG = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(up7)\n",
        "    \n",
        "    return Model(InIMG, OutIMG)\n",
        "    \n",
        "def build_discriminator():\n",
        "\n",
        "    A = Input(shape=img_shape)\n",
        "    B = Input(shape=img_shape)\n",
        "\n",
        "    AB = Concatenate(axis=-1)([A, B])\n",
        "\n",
        "    down1 = Conv2D(64, kernel_size=4, strides=2, padding='same')(AB)\n",
        "    down1 = LeakyReLU(alpha=0.2)(down1)\n",
        "    \n",
        "    down2 = Conv2D(128, kernel_size=4, strides=2, padding='same')(down1)\n",
        "    down2 = LeakyReLU(alpha=0.2)(down2)\n",
        "    down2 = BatchNormalization(momentum=0.8)(down2)\n",
        "    \n",
        "    down3 = Conv2D(256, kernel_size=4, strides=2, padding='same')(down2)\n",
        "    down3 = LeakyReLU(alpha=0.2)(down3)\n",
        "    down3 = BatchNormalization(momentum=0.8)(down3)\n",
        "    \n",
        "    down4 = Conv2D(512, kernel_size=4, strides=2, padding='same')(down3)\n",
        "    down4 = LeakyReLU(alpha=0.2)(down4)\n",
        "    down4 = BatchNormalization(momentum=0.8)(down4)\n",
        "    \n",
        "    discOut = Conv2D(1, kernel_size=4, strides=1, padding='same')(down4)\n",
        "    \n",
        "    return Model([A, B], discOut)\n",
        "  \n",
        "def train(epochs, sizeOfBatch=1, testAfter=50):\n",
        "\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    valid = np.ones((sizeOfBatch,) + disc_outShape)\n",
        "    fake = np.zeros((sizeOfBatch,) + disc_outShape)\n",
        "    \n",
        "    for currEpoch in range(epochs):\n",
        "        for i, (A, B) in enumerate(load_train(sizeOfBatch)):\n",
        "\n",
        "            fakeIMG = generator.predict(B)\n",
        "\n",
        "            disc_loss_real = discriminator.train_on_batch([A, B], valid)\n",
        "            disc_loss_fake = discriminator.train_on_batch([fakeIMG, B], fake)\n",
        "            disc_loss = 0.5 * np.add(disc_loss_real, disc_loss_fake)\n",
        "\n",
        "            gen_loss = combined.train_on_batch([A, B], [valid, A])\n",
        "\n",
        "            elapsed_time = datetime.datetime.now() - start_time\n",
        "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (currEpoch, epochs,\n",
        "                                                                    i, numBatches,\n",
        "                                                                    disc_loss[0], 100*disc_loss[1],\n",
        "                                                                    gen_loss[0],\n",
        "                                                                    elapsed_time))\n",
        "\n",
        "            if i % testAfter == 0:\n",
        "                print(\"Testing Image at Iteration No.\"+str(i))\n",
        "                test(currEpoch, i)\n",
        "\n",
        "def test(epoch, batch):\n",
        "    imgdir = '/content/drive/My Drive/ML Project/'\n",
        "    os.makedirs(imgdir + 'imagesFin/%s' % dataset_name, exist_ok=True)\n",
        "    \n",
        "    imgs_A, imgs_B = load_test()\n",
        "    fake_A = generator.predict(imgs_B)\n",
        "\n",
        "    fake = np.concatenate([fake_A, imgs_B, imgs_A])\n",
        "    A = np.concatenate([imgs_A, fake_A, imgs_B])\n",
        "    B = np.concatenate([imgs_B, fake_A, imgs_A])\n",
        "    print(fake_A.shape)\n",
        "    # Rescale images 0 - 1\n",
        "    fake = 0.5 * fake + 0.5\n",
        "    A = 0.5 * A + 0.5\n",
        "    B = 0.5 * B + 0.5\n",
        "    \n",
        "    fake1 = (fake[0]*255)\n",
        "    A1 = A[0]*255\n",
        "    B1 = B[0]*255\n",
        "    Image.fromarray(fake1.astype('uint8')).save(imgdir + \"imagesFin/%s/%d_%d_fake.png\" % (dataset_name, epoch, batch))\n",
        "    Image.fromarray(A1.astype('uint8')).save(imgdir + \"imagesFin/%s/%d_%d_A.png\" % (dataset_name, epoch, batch))\n",
        "    Image.fromarray(B1.astype('uint8')).save(imgdir + \"imagesFin/%s/%d_%d_B.png\" % (dataset_name, epoch, batch))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_EM0rBmEwQR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initialize the Model"
      ]
    },
    {
      "metadata": {
        "id": "Prt5-J6n-NU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "img_rows = 256\n",
        "img_cols = 256\n",
        "channels = 3\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "img_res=(img_rows, img_cols)\n",
        "dataset_name = 'ML_Dataset_5'\n",
        "\n",
        "imgdir = '/content/drive/My Drive/Colab Notebooks/CycleGANs'\n",
        "\n",
        "path = glob(imgdir + '/%s/train/*' % (dataset_name))\n",
        "numBatches = int(len(path))\n",
        "patch = int(img_rows / 2**4)\n",
        "disc_outShape = (patch, patch, 1)\n",
        "\n",
        "optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='mse',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "generator = build_generator()\n",
        "\n",
        "img_A = Input(shape=img_shape)\n",
        "img_B = Input(shape=img_shape)\n",
        "\n",
        "fake_A = generator(img_B)\n",
        "discriminator.trainable = False\n",
        "valid = discriminator([fake_A, img_B])\n",
        "\n",
        "combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
        "combined.compile(loss=['mse', 'mae'],\n",
        "                      loss_weights=[1, 100],\n",
        "                      optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6YxyMPhQE_j5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training and Testing the Model"
      ]
    },
    {
      "metadata": {
        "id": "b9Y2_fSL6XDa",
        "colab_type": "code",
        "outputId": "2e134122-9eed-47f2-e43c-3de39dd0704d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7170
        }
      },
      "cell_type": "code",
      "source": [
        "train(epochs=10, sizeOfBatch=1, testAfter=50)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/10] [Batch 0/134] [D loss: 3.634269, acc:  16%] [G loss: 73.228333] time: 0:01:06.653315\n",
            "Testing Image at Iteration No.0\n",
            "(1, 256, 256, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/10] [Batch 1/134] [D loss: 9.263795, acc:  17%] [G loss: 57.153648] time: 0:01:07.219920\n",
            "[Epoch 0/10] [Batch 2/134] [D loss: 8.986752, acc:  21%] [G loss: 50.083134] time: 0:01:07.583372\n",
            "[Epoch 0/10] [Batch 3/134] [D loss: 7.457245, acc:  19%] [G loss: 51.428093] time: 0:01:07.939492\n",
            "[Epoch 0/10] [Batch 4/134] [D loss: 3.634562, acc:  18%] [G loss: 37.666862] time: 0:01:08.292941\n",
            "[Epoch 0/10] [Batch 5/134] [D loss: 2.628344, acc:  26%] [G loss: 36.574558] time: 0:01:08.645688\n",
            "[Epoch 0/10] [Batch 6/134] [D loss: 2.583096, acc:  22%] [G loss: 34.790089] time: 0:01:09.003503\n",
            "[Epoch 0/10] [Batch 7/134] [D loss: 3.333271, acc:  25%] [G loss: 29.965353] time: 0:01:09.355869\n",
            "[Epoch 0/10] [Batch 8/134] [D loss: 2.045856, acc:  51%] [G loss: 25.767807] time: 0:01:09.710903\n",
            "[Epoch 0/10] [Batch 9/134] [D loss: 1.462942, acc:  30%] [G loss: 22.385681] time: 0:01:10.069833\n",
            "[Epoch 0/10] [Batch 10/134] [D loss: 0.895923, acc:  48%] [G loss: 20.873041] time: 0:01:10.429182\n",
            "[Epoch 0/10] [Batch 11/134] [D loss: 0.825154, acc:  43%] [G loss: 22.354568] time: 0:01:10.790585\n",
            "[Epoch 0/10] [Batch 12/134] [D loss: 0.714829, acc:  50%] [G loss: 22.076595] time: 0:01:11.152352\n",
            "[Epoch 0/10] [Batch 13/134] [D loss: 0.700730, acc:  51%] [G loss: 16.744238] time: 0:01:11.508404\n",
            "[Epoch 0/10] [Batch 14/134] [D loss: 0.531418, acc:  43%] [G loss: 15.350186] time: 0:01:11.862162\n",
            "[Epoch 0/10] [Batch 15/134] [D loss: 0.532483, acc:  46%] [G loss: 14.361736] time: 0:01:12.222905\n",
            "[Epoch 0/10] [Batch 16/134] [D loss: 0.689248, acc:  46%] [G loss: 16.506922] time: 0:01:12.577121\n",
            "[Epoch 0/10] [Batch 17/134] [D loss: 0.571503, acc:  53%] [G loss: 15.978048] time: 0:01:12.933273\n",
            "[Epoch 0/10] [Batch 18/134] [D loss: 0.554455, acc:  54%] [G loss: 18.549707] time: 0:01:13.291631\n",
            "[Epoch 0/10] [Batch 19/134] [D loss: 0.613675, acc:  54%] [G loss: 13.335826] time: 0:01:13.652091\n",
            "[Epoch 0/10] [Batch 20/134] [D loss: 0.568474, acc:  54%] [G loss: 16.484312] time: 0:01:14.008723\n",
            "[Epoch 0/10] [Batch 21/134] [D loss: 0.656132, acc:  53%] [G loss: 18.659931] time: 0:01:14.367094\n",
            "[Epoch 0/10] [Batch 22/134] [D loss: 0.581033, acc:  44%] [G loss: 13.639590] time: 0:01:14.726633\n",
            "[Epoch 0/10] [Batch 23/134] [D loss: 0.537300, acc:  55%] [G loss: 15.536682] time: 0:01:15.076910\n",
            "[Epoch 0/10] [Batch 24/134] [D loss: 0.499784, acc:  43%] [G loss: 17.064196] time: 0:01:15.442773\n",
            "[Epoch 0/10] [Batch 25/134] [D loss: 0.641218, acc:  40%] [G loss: 15.255417] time: 0:01:15.794244\n",
            "[Epoch 0/10] [Batch 26/134] [D loss: 0.457545, acc:  51%] [G loss: 13.395641] time: 0:01:16.157575\n",
            "[Epoch 0/10] [Batch 27/134] [D loss: 0.461719, acc:  55%] [G loss: 13.704078] time: 0:01:16.520257\n",
            "[Epoch 0/10] [Batch 28/134] [D loss: 0.423571, acc:  42%] [G loss: 14.509978] time: 0:01:16.882896\n",
            "[Epoch 0/10] [Batch 29/134] [D loss: 0.486424, acc:  52%] [G loss: 14.290505] time: 0:01:17.236810\n",
            "[Epoch 0/10] [Batch 30/134] [D loss: 0.511479, acc:  54%] [G loss: 15.448822] time: 0:01:17.595866\n",
            "[Epoch 0/10] [Batch 31/134] [D loss: 0.475955, acc:  52%] [G loss: 13.843753] time: 0:01:17.955857\n",
            "[Epoch 0/10] [Batch 32/134] [D loss: 0.479901, acc:  59%] [G loss: 15.135924] time: 0:01:18.308291\n",
            "[Epoch 0/10] [Batch 33/134] [D loss: 0.423150, acc:  50%] [G loss: 15.180440] time: 0:01:18.664643\n",
            "[Epoch 0/10] [Batch 34/134] [D loss: 0.468123, acc:  31%] [G loss: 12.415826] time: 0:01:19.020469\n",
            "[Epoch 0/10] [Batch 35/134] [D loss: 0.422692, acc:  49%] [G loss: 15.116371] time: 0:01:19.372154\n",
            "[Epoch 0/10] [Batch 36/134] [D loss: 0.396232, acc:  55%] [G loss: 14.428872] time: 0:01:19.730607\n",
            "[Epoch 0/10] [Batch 37/134] [D loss: 0.396108, acc:  52%] [G loss: 10.879304] time: 0:01:20.092289\n",
            "[Epoch 0/10] [Batch 38/134] [D loss: 0.369289, acc:  56%] [G loss: 11.862609] time: 0:01:20.447548\n",
            "[Epoch 0/10] [Batch 39/134] [D loss: 0.456115, acc:  61%] [G loss: 15.327947] time: 0:01:20.807739\n",
            "[Epoch 0/10] [Batch 40/134] [D loss: 0.370076, acc:  41%] [G loss: 11.176304] time: 0:01:21.163834\n",
            "[Epoch 0/10] [Batch 41/134] [D loss: 0.412315, acc:  49%] [G loss: 11.102455] time: 0:01:21.516347\n",
            "[Epoch 0/10] [Batch 42/134] [D loss: 0.393029, acc:  47%] [G loss: 13.396138] time: 0:01:21.871553\n",
            "[Epoch 0/10] [Batch 43/134] [D loss: 0.368832, acc:  44%] [G loss: 14.120022] time: 0:01:22.232071\n",
            "[Epoch 0/10] [Batch 44/134] [D loss: 0.370990, acc:  48%] [G loss: 12.450056] time: 0:01:22.585118\n",
            "[Epoch 0/10] [Batch 45/134] [D loss: 0.418276, acc:  35%] [G loss: 12.674806] time: 0:01:22.949564\n",
            "[Epoch 0/10] [Batch 46/134] [D loss: 0.392331, acc:  33%] [G loss: 13.084545] time: 0:01:23.304920\n",
            "[Epoch 0/10] [Batch 47/134] [D loss: 0.380876, acc:  52%] [G loss: 10.389800] time: 0:01:23.660382\n",
            "[Epoch 0/10] [Batch 48/134] [D loss: 0.402740, acc:  53%] [G loss: 11.657728] time: 0:01:24.023621\n",
            "[Epoch 0/10] [Batch 49/134] [D loss: 0.406483, acc:  52%] [G loss: 12.251785] time: 0:01:24.376992\n",
            "[Epoch 0/10] [Batch 50/134] [D loss: 0.414324, acc:  51%] [G loss: 14.835044] time: 0:01:24.731519\n",
            "Testing Image at Iteration No.50\n",
            "(1, 256, 256, 3)\n",
            "[Epoch 0/10] [Batch 51/134] [D loss: 0.413180, acc:  56%] [G loss: 12.732101] time: 0:01:25.255690\n",
            "[Epoch 0/10] [Batch 52/134] [D loss: 0.440479, acc:  63%] [G loss: 11.745044] time: 0:01:25.611013\n",
            "[Epoch 0/10] [Batch 53/134] [D loss: 0.359150, acc:  44%] [G loss: 14.802926] time: 0:01:25.964217\n",
            "[Epoch 0/10] [Batch 54/134] [D loss: 0.345096, acc:  47%] [G loss: 10.921105] time: 0:01:26.319302\n",
            "[Epoch 0/10] [Batch 55/134] [D loss: 0.352226, acc:  60%] [G loss: 10.571352] time: 0:01:26.677793\n",
            "[Epoch 0/10] [Batch 56/134] [D loss: 0.350331, acc:  53%] [G loss: 11.829970] time: 0:01:27.032475\n",
            "[Epoch 0/10] [Batch 57/134] [D loss: 0.340627, acc:  59%] [G loss: 9.881120] time: 0:01:27.391633\n",
            "[Epoch 0/10] [Batch 58/134] [D loss: 0.353499, acc:  36%] [G loss: 12.558619] time: 0:01:27.743290\n",
            "[Epoch 0/10] [Batch 59/134] [D loss: 0.370209, acc:  41%] [G loss: 12.904454] time: 0:01:28.101557\n",
            "[Epoch 0/10] [Batch 60/134] [D loss: 0.332813, acc:  43%] [G loss: 9.604395] time: 0:01:28.457290\n",
            "[Epoch 0/10] [Batch 61/134] [D loss: 0.352665, acc:  39%] [G loss: 11.226992] time: 0:01:28.813147\n",
            "[Epoch 0/10] [Batch 62/134] [D loss: 0.321721, acc:  51%] [G loss: 12.152587] time: 0:01:29.174179\n",
            "[Epoch 0/10] [Batch 63/134] [D loss: 0.344270, acc:  57%] [G loss: 12.811904] time: 0:01:29.533580\n",
            "[Epoch 0/10] [Batch 64/134] [D loss: 0.354215, acc:  65%] [G loss: 10.569680] time: 0:01:29.886952\n",
            "[Epoch 0/10] [Batch 65/134] [D loss: 0.349622, acc:  61%] [G loss: 11.167060] time: 0:01:30.241634\n",
            "[Epoch 0/10] [Batch 66/134] [D loss: 0.339302, acc:  55%] [G loss: 12.472596] time: 0:01:30.599502\n",
            "[Epoch 0/10] [Batch 67/134] [D loss: 0.326522, acc:  46%] [G loss: 8.982754] time: 0:01:30.951392\n",
            "[Epoch 0/10] [Batch 68/134] [D loss: 0.314862, acc:  54%] [G loss: 10.480331] time: 0:01:31.323390\n",
            "[Epoch 0/10] [Batch 69/134] [D loss: 0.320903, acc:  41%] [G loss: 10.421724] time: 0:01:31.683130\n",
            "[Epoch 0/10] [Batch 70/134] [D loss: 0.324088, acc:  52%] [G loss: 11.217680] time: 0:01:32.036070\n",
            "[Epoch 0/10] [Batch 71/134] [D loss: 0.333923, acc:  59%] [G loss: 11.350594] time: 0:01:32.389743\n",
            "[Epoch 0/10] [Batch 72/134] [D loss: 0.372243, acc:  63%] [G loss: 11.098218] time: 0:01:32.749338\n",
            "[Epoch 0/10] [Batch 73/134] [D loss: 0.328423, acc:  57%] [G loss: 11.105209] time: 0:01:33.107900\n",
            "[Epoch 0/10] [Batch 74/134] [D loss: 0.340250, acc:  62%] [G loss: 11.624473] time: 0:01:33.459313\n",
            "[Epoch 0/10] [Batch 75/134] [D loss: 0.444855, acc:  68%] [G loss: 11.321475] time: 0:01:33.818062\n",
            "[Epoch 0/10] [Batch 76/134] [D loss: 0.383240, acc:  66%] [G loss: 12.579419] time: 0:01:34.174680\n",
            "[Epoch 0/10] [Batch 77/134] [D loss: 0.389682, acc:  62%] [G loss: 11.540939] time: 0:01:34.535054\n",
            "[Epoch 0/10] [Batch 78/134] [D loss: 0.361028, acc:  60%] [G loss: 14.147409] time: 0:01:34.898902\n",
            "[Epoch 0/10] [Batch 79/134] [D loss: 0.387277, acc:  66%] [G loss: 12.902328] time: 0:01:35.257501\n",
            "[Epoch 0/10] [Batch 80/134] [D loss: 0.549005, acc:  70%] [G loss: 10.573643] time: 0:01:35.611710\n",
            "[Epoch 0/10] [Batch 81/134] [D loss: 0.356438, acc:  60%] [G loss: 11.832529] time: 0:01:35.972732\n",
            "[Epoch 0/10] [Batch 82/134] [D loss: 0.316817, acc:  49%] [G loss: 10.679688] time: 0:01:36.337402\n",
            "[Epoch 0/10] [Batch 83/134] [D loss: 0.327667, acc:  37%] [G loss: 12.161320] time: 0:01:36.693486\n",
            "[Epoch 0/10] [Batch 84/134] [D loss: 0.322954, acc:  34%] [G loss: 10.212035] time: 0:01:37.054478\n",
            "[Epoch 0/10] [Batch 85/134] [D loss: 0.315222, acc:  57%] [G loss: 10.880868] time: 0:01:37.415240\n",
            "[Epoch 0/10] [Batch 86/134] [D loss: 0.319658, acc:  54%] [G loss: 11.980680] time: 0:01:37.773191\n",
            "[Epoch 0/10] [Batch 87/134] [D loss: 0.304168, acc:  54%] [G loss: 9.900489] time: 0:01:38.139469\n",
            "[Epoch 0/10] [Batch 88/134] [D loss: 0.319520, acc:  54%] [G loss: 12.124137] time: 0:01:38.491480\n",
            "[Epoch 0/10] [Batch 89/134] [D loss: 0.306009, acc:  46%] [G loss: 15.508342] time: 0:01:38.845831\n",
            "[Epoch 0/10] [Batch 90/134] [D loss: 0.299062, acc:  45%] [G loss: 12.885557] time: 0:01:39.203945\n",
            "[Epoch 0/10] [Batch 91/134] [D loss: 0.322221, acc:  47%] [G loss: 12.037700] time: 0:01:39.562946\n",
            "[Epoch 0/10] [Batch 92/134] [D loss: 0.298262, acc:  40%] [G loss: 11.289349] time: 0:01:39.915535\n",
            "[Epoch 0/10] [Batch 93/134] [D loss: 0.320389, acc:  28%] [G loss: 10.069654] time: 0:01:40.276676\n",
            "[Epoch 0/10] [Batch 94/134] [D loss: 0.312107, acc:  37%] [G loss: 11.604258] time: 0:01:40.635624\n",
            "[Epoch 0/10] [Batch 95/134] [D loss: 0.320116, acc:  52%] [G loss: 9.399227] time: 0:01:40.988578\n",
            "[Epoch 0/10] [Batch 96/134] [D loss: 0.305604, acc:  59%] [G loss: 10.199518] time: 0:01:41.345231\n",
            "[Epoch 0/10] [Batch 97/134] [D loss: 0.310299, acc:  46%] [G loss: 11.028509] time: 0:01:41.696608\n",
            "[Epoch 0/10] [Batch 98/134] [D loss: 0.309606, acc:  61%] [G loss: 11.863407] time: 0:01:42.050527\n",
            "[Epoch 0/10] [Batch 99/134] [D loss: 0.395194, acc:  69%] [G loss: 8.967268] time: 0:01:42.407387\n",
            "[Epoch 0/10] [Batch 100/134] [D loss: 0.484845, acc:  71%] [G loss: 13.440717] time: 0:01:42.763691\n",
            "Testing Image at Iteration No.100\n",
            "(1, 256, 256, 3)\n",
            "[Epoch 0/10] [Batch 101/134] [D loss: 0.739895, acc:  67%] [G loss: 10.529172] time: 0:01:43.278109\n",
            "[Epoch 0/10] [Batch 102/134] [D loss: 0.815647, acc:  72%] [G loss: 11.334222] time: 0:01:43.643856\n",
            "[Epoch 0/10] [Batch 103/134] [D loss: 0.592323, acc:  72%] [G loss: 11.222901] time: 0:01:44.004138\n",
            "[Epoch 0/10] [Batch 104/134] [D loss: 0.343224, acc:  63%] [G loss: 9.907519] time: 0:01:44.358088\n",
            "[Epoch 0/10] [Batch 105/134] [D loss: 0.309614, acc:  33%] [G loss: 10.101666] time: 0:01:44.712581\n",
            "[Epoch 0/10] [Batch 106/134] [D loss: 0.326508, acc:  41%] [G loss: 12.590555] time: 0:01:45.068439\n",
            "[Epoch 0/10] [Batch 107/134] [D loss: 0.300035, acc:  41%] [G loss: 11.852555] time: 0:01:45.421256\n",
            "[Epoch 0/10] [Batch 108/134] [D loss: 0.293399, acc:  39%] [G loss: 11.289308] time: 0:01:45.789337\n",
            "[Epoch 0/10] [Batch 109/134] [D loss: 0.309740, acc:  31%] [G loss: 9.884256] time: 0:01:46.143274\n",
            "[Epoch 0/10] [Batch 110/134] [D loss: 0.318703, acc:  50%] [G loss: 10.130020] time: 0:01:46.496528\n",
            "[Epoch 0/10] [Batch 111/134] [D loss: 0.303623, acc:  39%] [G loss: 15.247058] time: 0:01:46.859078\n",
            "[Epoch 0/10] [Batch 112/134] [D loss: 0.311570, acc:  32%] [G loss: 10.198936] time: 0:01:47.213800\n",
            "[Epoch 0/10] [Batch 113/134] [D loss: 0.311450, acc:  35%] [G loss: 11.682053] time: 0:01:47.565862\n",
            "[Epoch 0/10] [Batch 114/134] [D loss: 0.301504, acc:  35%] [G loss: 12.135461] time: 0:01:47.923259\n",
            "[Epoch 0/10] [Batch 115/134] [D loss: 0.299932, acc:  38%] [G loss: 13.235312] time: 0:01:48.278189\n",
            "[Epoch 0/10] [Batch 116/134] [D loss: 0.294414, acc:  57%] [G loss: 10.064599] time: 0:01:48.628924\n",
            "[Epoch 0/10] [Batch 117/134] [D loss: 0.297297, acc:  54%] [G loss: 10.894251] time: 0:01:48.986347\n",
            "[Epoch 0/10] [Batch 118/134] [D loss: 0.297546, acc:  54%] [G loss: 12.415713] time: 0:01:49.339993\n",
            "[Epoch 0/10] [Batch 119/134] [D loss: 0.289398, acc:  60%] [G loss: 10.209889] time: 0:01:49.694926\n",
            "[Epoch 0/10] [Batch 120/134] [D loss: 0.293741, acc:  57%] [G loss: 10.073853] time: 0:01:50.053989\n",
            "[Epoch 0/10] [Batch 121/134] [D loss: 0.283901, acc:  57%] [G loss: 9.932502] time: 0:01:50.406457\n",
            "[Epoch 0/10] [Batch 122/134] [D loss: 0.296880, acc:  54%] [G loss: 9.651335] time: 0:01:50.762959\n",
            "[Epoch 0/10] [Batch 123/134] [D loss: 0.278395, acc:  58%] [G loss: 12.050154] time: 0:01:51.124856\n",
            "[Epoch 0/10] [Batch 124/134] [D loss: 0.294809, acc:  60%] [G loss: 10.272986] time: 0:01:51.481621\n",
            "[Epoch 0/10] [Batch 125/134] [D loss: 0.274131, acc:  57%] [G loss: 10.257841] time: 0:01:51.844727\n",
            "[Epoch 0/10] [Batch 126/134] [D loss: 0.313609, acc:  62%] [G loss: 9.304315] time: 0:01:52.209658\n",
            "[Epoch 0/10] [Batch 127/134] [D loss: 0.284177, acc:  64%] [G loss: 10.250493] time: 0:01:52.561356\n",
            "[Epoch 0/10] [Batch 128/134] [D loss: 0.306498, acc:  65%] [G loss: 10.008604] time: 0:01:52.926196\n",
            "[Epoch 0/10] [Batch 129/134] [D loss: 0.297741, acc:  58%] [G loss: 11.029009] time: 0:01:53.284433\n",
            "[Epoch 0/10] [Batch 130/134] [D loss: 0.299556, acc:  47%] [G loss: 10.761865] time: 0:01:53.636786\n",
            "[Epoch 0/10] [Batch 131/134] [D loss: 0.303998, acc:  58%] [G loss: 12.874573] time: 0:01:53.991682\n",
            "[Epoch 0/10] [Batch 132/134] [D loss: 0.295196, acc:  55%] [G loss: 9.518311] time: 0:01:54.346535\n",
            "[Epoch 1/10] [Batch 0/134] [D loss: 0.315250, acc:  59%] [G loss: 10.357294] time: 0:01:54.702237\n",
            "Testing Image at Iteration No.0\n",
            "(1, 256, 256, 3)\n",
            "[Epoch 1/10] [Batch 1/134] [D loss: 0.290141, acc:  53%] [G loss: 10.836611] time: 0:01:55.216810\n",
            "[Epoch 1/10] [Batch 2/134] [D loss: 0.271996, acc:  47%] [G loss: 13.128386] time: 0:01:55.576342\n",
            "[Epoch 1/10] [Batch 3/134] [D loss: 0.288566, acc:  50%] [G loss: 12.272213] time: 0:01:55.930818\n",
            "[Epoch 1/10] [Batch 4/134] [D loss: 0.272757, acc:  44%] [G loss: 10.806568] time: 0:01:56.292423\n",
            "[Epoch 1/10] [Batch 5/134] [D loss: 0.327377, acc:  33%] [G loss: 11.512477] time: 0:01:56.649996\n",
            "[Epoch 1/10] [Batch 6/134] [D loss: 0.364141, acc:  26%] [G loss: 9.726142] time: 0:01:57.009566\n",
            "[Epoch 1/10] [Batch 7/134] [D loss: 0.315693, acc:  36%] [G loss: 9.126358] time: 0:01:57.361879\n",
            "[Epoch 1/10] [Batch 8/134] [D loss: 0.295559, acc:  32%] [G loss: 9.777929] time: 0:01:57.719887\n",
            "[Epoch 1/10] [Batch 9/134] [D loss: 0.289122, acc:  34%] [G loss: 9.394642] time: 0:01:58.078718\n",
            "[Epoch 1/10] [Batch 10/134] [D loss: 0.320852, acc:  31%] [G loss: 14.296912] time: 0:01:58.429453\n",
            "[Epoch 1/10] [Batch 11/134] [D loss: 0.278032, acc:  42%] [G loss: 10.787947] time: 0:01:58.787778\n",
            "[Epoch 1/10] [Batch 12/134] [D loss: 0.276874, acc:  44%] [G loss: 10.727391] time: 0:01:59.154452\n",
            "[Epoch 1/10] [Batch 13/134] [D loss: 0.283213, acc:  46%] [G loss: 10.600869] time: 0:01:59.507992\n",
            "[Epoch 1/10] [Batch 14/134] [D loss: 0.302468, acc:  61%] [G loss: 9.081036] time: 0:01:59.867334\n",
            "[Epoch 1/10] [Batch 15/134] [D loss: 0.325204, acc:  67%] [G loss: 9.205661] time: 0:02:00.222188\n",
            "[Epoch 1/10] [Batch 16/134] [D loss: 0.289825, acc:  62%] [G loss: 11.676063] time: 0:02:00.574575\n",
            "[Epoch 1/10] [Batch 17/134] [D loss: 0.278192, acc:  55%] [G loss: 8.640040] time: 0:02:00.926656\n",
            "[Epoch 1/10] [Batch 18/134] [D loss: 0.271298, acc:  44%] [G loss: 9.328243] time: 0:02:01.284258\n",
            "[Epoch 1/10] [Batch 19/134] [D loss: 0.275969, acc:  45%] [G loss: 12.133585] time: 0:02:01.643614\n",
            "[Epoch 1/10] [Batch 20/134] [D loss: 0.287048, acc:  34%] [G loss: 10.277646] time: 0:02:01.996965\n",
            "[Epoch 1/10] [Batch 21/134] [D loss: 0.287727, acc:  35%] [G loss: 8.724016] time: 0:02:02.354912\n",
            "[Epoch 1/10] [Batch 22/134] [D loss: 0.270452, acc:  48%] [G loss: 9.154243] time: 0:02:02.708212\n",
            "[Epoch 1/10] [Batch 23/134] [D loss: 0.296856, acc:  64%] [G loss: 10.349557] time: 0:02:03.061139\n",
            "[Epoch 1/10] [Batch 24/134] [D loss: 0.332596, acc:  64%] [G loss: 11.234121] time: 0:02:03.414339\n",
            "[Epoch 1/10] [Batch 25/134] [D loss: 0.357048, acc:  68%] [G loss: 9.845490] time: 0:02:03.766007\n",
            "[Epoch 1/10] [Batch 26/134] [D loss: 0.371221, acc:  69%] [G loss: 9.931618] time: 0:02:04.124919\n",
            "[Epoch 1/10] [Batch 27/134] [D loss: 0.532311, acc:  68%] [G loss: 10.662664] time: 0:02:04.478989\n",
            "[Epoch 1/10] [Batch 28/134] [D loss: 0.546770, acc:  67%] [G loss: 9.914055] time: 0:02:04.834593\n",
            "[Epoch 1/10] [Batch 29/134] [D loss: 0.844990, acc:  72%] [G loss: 11.234684] time: 0:02:05.188442\n",
            "[Epoch 1/10] [Batch 30/134] [D loss: 0.625997, acc:  69%] [G loss: 10.527940] time: 0:02:05.545856\n",
            "[Epoch 1/10] [Batch 31/134] [D loss: 0.300382, acc:  58%] [G loss: 8.263371] time: 0:02:05.896078\n",
            "[Epoch 1/10] [Batch 32/134] [D loss: 0.292253, acc:  37%] [G loss: 11.211381] time: 0:02:06.254406\n",
            "[Epoch 1/10] [Batch 33/134] [D loss: 0.330840, acc:  25%] [G loss: 9.638504] time: 0:02:06.611961\n",
            "[Epoch 1/10] [Batch 34/134] [D loss: 0.291104, acc:  39%] [G loss: 7.887321] time: 0:02:06.971801\n",
            "[Epoch 1/10] [Batch 35/134] [D loss: 0.280862, acc:  34%] [G loss: 9.214288] time: 0:02:07.326004\n",
            "[Epoch 1/10] [Batch 36/134] [D loss: 0.285168, acc:  37%] [G loss: 11.318398] time: 0:02:07.683500\n",
            "[Epoch 1/10] [Batch 37/134] [D loss: 0.267779, acc:  56%] [G loss: 7.519163] time: 0:02:08.041574\n",
            "[Epoch 1/10] [Batch 38/134] [D loss: 0.276737, acc:  59%] [G loss: 9.091061] time: 0:02:08.400547\n",
            "[Epoch 1/10] [Batch 39/134] [D loss: 0.270561, acc:  57%] [G loss: 8.921714] time: 0:02:08.753856\n",
            "[Epoch 1/10] [Batch 40/134] [D loss: 0.267208, acc:  53%] [G loss: 8.242229] time: 0:02:09.109983\n",
            "[Epoch 1/10] [Batch 41/134] [D loss: 0.266184, acc:  58%] [G loss: 9.712492] time: 0:02:09.477166\n",
            "[Epoch 1/10] [Batch 42/134] [D loss: 0.282352, acc:  66%] [G loss: 9.194007] time: 0:02:09.829488\n",
            "[Epoch 1/10] [Batch 43/134] [D loss: 0.284478, acc:  57%] [G loss: 11.014053] time: 0:02:10.182625\n",
            "[Epoch 1/10] [Batch 44/134] [D loss: 0.279424, acc:  59%] [G loss: 9.310796] time: 0:02:10.539581\n",
            "[Epoch 1/10] [Batch 45/134] [D loss: 0.274667, acc:  41%] [G loss: 7.910568] time: 0:02:10.892466\n",
            "[Epoch 1/10] [Batch 46/134] [D loss: 0.270382, acc:  40%] [G loss: 7.869547] time: 0:02:11.248478\n",
            "[Epoch 1/10] [Batch 47/134] [D loss: 0.270330, acc:  49%] [G loss: 9.211867] time: 0:02:11.612090\n",
            "[Epoch 1/10] [Batch 48/134] [D loss: 0.278196, acc:  44%] [G loss: 9.632134] time: 0:02:11.969510\n",
            "[Epoch 1/10] [Batch 49/134] [D loss: 0.271157, acc:  54%] [G loss: 8.012604] time: 0:02:12.320916\n",
            "[Epoch 1/10] [Batch 50/134] [D loss: 0.273638, acc:  48%] [G loss: 10.157911] time: 0:02:12.680686\n",
            "Testing Image at Iteration No.50\n",
            "(1, 256, 256, 3)\n",
            "[Epoch 1/10] [Batch 51/134] [D loss: 0.282402, acc:  34%] [G loss: 10.209654] time: 0:02:13.195855\n",
            "[Epoch 1/10] [Batch 52/134] [D loss: 0.296822, acc:  34%] [G loss: 9.173973] time: 0:02:13.555383\n",
            "[Epoch 1/10] [Batch 53/134] [D loss: 0.277112, acc:  35%] [G loss: 10.778971] time: 0:02:13.906380\n",
            "[Epoch 1/10] [Batch 54/134] [D loss: 0.271538, acc:  38%] [G loss: 9.926380] time: 0:02:14.262865\n",
            "[Epoch 1/10] [Batch 55/134] [D loss: 0.288056, acc:  57%] [G loss: 9.192682] time: 0:02:14.622702\n",
            "[Epoch 1/10] [Batch 56/134] [D loss: 0.297761, acc:  69%] [G loss: 10.893740] time: 0:02:14.978616\n",
            "[Epoch 1/10] [Batch 57/134] [D loss: 0.295068, acc:  61%] [G loss: 9.950730] time: 0:02:15.331361\n",
            "[Epoch 1/10] [Batch 58/134] [D loss: 0.334168, acc:  71%] [G loss: 10.443643] time: 0:02:15.689243\n",
            "[Epoch 1/10] [Batch 59/134] [D loss: 0.320116, acc:  70%] [G loss: 9.676804] time: 0:02:16.042846\n",
            "[Epoch 1/10] [Batch 60/134] [D loss: 0.321506, acc:  66%] [G loss: 10.038593] time: 0:02:16.393790\n",
            "[Epoch 1/10] [Batch 61/134] [D loss: 0.320788, acc:  67%] [G loss: 9.137363] time: 0:02:16.747918\n",
            "[Epoch 1/10] [Batch 62/134] [D loss: 0.273644, acc:  51%] [G loss: 8.419983] time: 0:02:17.103106\n",
            "[Epoch 1/10] [Batch 63/134] [D loss: 0.276498, acc:  52%] [G loss: 9.924481] time: 0:02:17.457370\n",
            "[Epoch 1/10] [Batch 64/134] [D loss: 0.278255, acc:  59%] [G loss: 8.448510] time: 0:02:17.821509\n",
            "[Epoch 1/10] [Batch 65/134] [D loss: 0.266616, acc:  56%] [G loss: 8.188070] time: 0:02:18.175087\n",
            "[Epoch 1/10] [Batch 66/134] [D loss: 0.289970, acc:  62%] [G loss: 8.889405] time: 0:02:18.528373\n",
            "[Epoch 1/10] [Batch 67/134] [D loss: 0.275797, acc:  56%] [G loss: 7.937207] time: 0:02:18.887510\n",
            "[Epoch 1/10] [Batch 68/134] [D loss: 0.263990, acc:  54%] [G loss: 8.987734] time: 0:02:19.244271\n",
            "[Epoch 1/10] [Batch 69/134] [D loss: 0.301327, acc:  33%] [G loss: 9.128925] time: 0:02:19.605372\n",
            "[Epoch 1/10] [Batch 70/134] [D loss: 0.285512, acc:  37%] [G loss: 7.455900] time: 0:02:19.962353\n",
            "[Epoch 1/10] [Batch 71/134] [D loss: 0.286629, acc:  36%] [G loss: 9.539423] time: 0:02:20.314717\n",
            "[Epoch 1/10] [Batch 72/134] [D loss: 0.271071, acc:  49%] [G loss: 9.809794] time: 0:02:20.667893\n",
            "[Epoch 1/10] [Batch 73/134] [D loss: 0.276497, acc:  35%] [G loss: 8.289107] time: 0:02:21.022157\n",
            "[Epoch 1/10] [Batch 74/134] [D loss: 0.276657, acc:  34%] [G loss: 11.224126] time: 0:02:21.375828\n",
            "[Epoch 1/10] [Batch 75/134] [D loss: 0.343819, acc:  28%] [G loss: 10.687374] time: 0:02:21.731251\n",
            "[Epoch 1/10] [Batch 76/134] [D loss: 0.338977, acc:  32%] [G loss: 8.674558] time: 0:02:22.086842\n",
            "[Epoch 1/10] [Batch 77/134] [D loss: 0.400163, acc:  25%] [G loss: 9.693079] time: 0:02:22.444756\n",
            "[Epoch 1/10] [Batch 78/134] [D loss: 0.680760, acc:  17%] [G loss: 9.102278] time: 0:02:22.800671\n",
            "[Epoch 1/10] [Batch 79/134] [D loss: 0.842876, acc:  15%] [G loss: 9.837155] time: 0:02:23.154886\n",
            "[Epoch 1/10] [Batch 80/134] [D loss: 0.853096, acc:  14%] [G loss: 9.230819] time: 0:02:23.513002\n",
            "[Epoch 1/10] [Batch 81/134] [D loss: 0.440274, acc:  28%] [G loss: 8.913635] time: 0:02:23.867770\n",
            "[Epoch 1/10] [Batch 82/134] [D loss: 0.285133, acc:  45%] [G loss: 10.056402] time: 0:02:24.230955\n",
            "[Epoch 1/10] [Batch 83/134] [D loss: 0.306475, acc:  64%] [G loss: 7.894559] time: 0:02:24.584234\n",
            "[Epoch 1/10] [Batch 84/134] [D loss: 0.311499, acc:  64%] [G loss: 8.907251] time: 0:02:24.943808\n",
            "[Epoch 1/10] [Batch 85/134] [D loss: 0.338847, acc:  70%] [G loss: 9.002800] time: 0:02:25.301848\n",
            "[Epoch 1/10] [Batch 86/134] [D loss: 0.299905, acc:  66%] [G loss: 10.139794] time: 0:02:25.656447\n",
            "[Epoch 1/10] [Batch 87/134] [D loss: 0.296185, acc:  69%] [G loss: 8.611005] time: 0:02:26.011823\n",
            "[Epoch 1/10] [Batch 88/134] [D loss: 0.272000, acc:  59%] [G loss: 9.695425] time: 0:02:26.366725\n",
            "[Epoch 1/10] [Batch 89/134] [D loss: 0.295617, acc:  64%] [G loss: 8.733813] time: 0:02:26.720553\n",
            "[Epoch 1/10] [Batch 90/134] [D loss: 0.288377, acc:  72%] [G loss: 13.036406] time: 0:02:27.077879\n",
            "[Epoch 1/10] [Batch 91/134] [D loss: 0.270374, acc:  50%] [G loss: 9.150869] time: 0:02:27.433553\n",
            "[Epoch 1/10] [Batch 92/134] [D loss: 0.271335, acc:  55%] [G loss: 8.152909] time: 0:02:27.790924\n",
            "[Epoch 1/10] [Batch 93/134] [D loss: 0.306496, acc:  64%] [G loss: 8.351059] time: 0:02:28.155490\n",
            "[Epoch 1/10] [Batch 94/134] [D loss: 0.387148, acc:  69%] [G loss: 9.045904] time: 0:02:28.509232\n",
            "[Epoch 1/10] [Batch 95/134] [D loss: 0.354935, acc:  69%] [G loss: 8.476927] time: 0:02:28.864224\n",
            "[Epoch 1/10] [Batch 96/134] [D loss: 0.303588, acc:  66%] [G loss: 13.610547] time: 0:02:29.219826\n",
            "[Epoch 1/10] [Batch 97/134] [D loss: 0.280573, acc:  64%] [G loss: 10.063332] time: 0:02:29.576365\n",
            "[Epoch 1/10] [Batch 98/134] [D loss: 0.271978, acc:  63%] [G loss: 10.635780] time: 0:02:29.937739\n",
            "[Epoch 1/10] [Batch 99/134] [D loss: 0.304938, acc:  62%] [G loss: 8.819492] time: 0:02:30.293620\n",
            "[Epoch 1/10] [Batch 100/134] [D loss: 0.286404, acc:  68%] [G loss: 10.502548] time: 0:02:30.649371\n",
            "Testing Image at Iteration No.100\n",
            "(1, 256, 256, 3)\n",
            "[Epoch 1/10] [Batch 101/134] [D loss: 0.270756, acc:  59%] [G loss: 9.087041] time: 0:02:31.164296\n",
            "[Epoch 1/10] [Batch 102/134] [D loss: 0.272103, acc:  30%] [G loss: 8.910927] time: 0:02:31.517757\n",
            "[Epoch 1/10] [Batch 103/134] [D loss: 0.270552, acc:  42%] [G loss: 9.107349] time: 0:02:31.871796\n",
            "[Epoch 1/10] [Batch 104/134] [D loss: 0.261759, acc:  48%] [G loss: 7.762113] time: 0:02:32.227714\n",
            "[Epoch 1/10] [Batch 105/134] [D loss: 0.280238, acc:  63%] [G loss: 7.050268] time: 0:02:32.586225\n",
            "[Epoch 1/10] [Batch 106/134] [D loss: 0.293391, acc:  63%] [G loss: 11.199900] time: 0:02:32.937217\n",
            "[Epoch 1/10] [Batch 107/134] [D loss: 0.274714, acc:  54%] [G loss: 8.952472] time: 0:02:33.301435\n",
            "[Epoch 1/10] [Batch 108/134] [D loss: 0.277319, acc:  34%] [G loss: 9.219523] time: 0:02:33.652798\n",
            "[Epoch 1/10] [Batch 109/134] [D loss: 0.274148, acc:  41%] [G loss: 11.129184] time: 0:02:34.011751\n",
            "[Epoch 1/10] [Batch 110/134] [D loss: 0.281233, acc:  57%] [G loss: 8.926718] time: 0:02:34.366823\n",
            "[Epoch 1/10] [Batch 111/134] [D loss: 0.306847, acc:  66%] [G loss: 7.483849] time: 0:02:34.720447\n",
            "[Epoch 1/10] [Batch 112/134] [D loss: 0.378553, acc:  68%] [G loss: 10.069023] time: 0:02:35.075746\n",
            "[Epoch 1/10] [Batch 113/134] [D loss: 0.504640, acc:  70%] [G loss: 12.952229] time: 0:02:35.438283\n",
            "[Epoch 1/10] [Batch 114/134] [D loss: 0.575757, acc:  70%] [G loss: 9.083317] time: 0:02:35.797012\n",
            "[Epoch 1/10] [Batch 115/134] [D loss: 0.582165, acc:  69%] [G loss: 9.436129] time: 0:02:36.154830\n",
            "[Epoch 1/10] [Batch 116/134] [D loss: 0.365750, acc:  68%] [G loss: 9.318852] time: 0:02:36.506226\n",
            "[Epoch 1/10] [Batch 117/134] [D loss: 0.274103, acc:  52%] [G loss: 7.740353] time: 0:02:36.866282\n",
            "[Epoch 1/10] [Batch 118/134] [D loss: 0.275262, acc:  42%] [G loss: 8.669312] time: 0:02:37.223576\n",
            "[Epoch 1/10] [Batch 119/134] [D loss: 0.272693, acc:  61%] [G loss: 9.768871] time: 0:02:37.575483\n",
            "[Epoch 1/10] [Batch 120/134] [D loss: 0.284479, acc:  64%] [G loss: 8.660073] time: 0:02:37.926144\n",
            "[Epoch 1/10] [Batch 121/134] [D loss: 0.308980, acc:  68%] [G loss: 8.904285] time: 0:02:38.280619\n",
            "[Epoch 1/10] [Batch 122/134] [D loss: 0.279515, acc:  59%] [G loss: 9.064495] time: 0:02:38.632476\n",
            "[Epoch 1/10] [Batch 123/134] [D loss: 0.280883, acc:  63%] [G loss: 8.028930] time: 0:02:38.986372\n",
            "[Epoch 1/10] [Batch 124/134] [D loss: 0.281482, acc:  63%] [G loss: 8.600729] time: 0:02:39.341824\n",
            "[Epoch 1/10] [Batch 125/134] [D loss: 0.270934, acc:  59%] [G loss: 7.710238] time: 0:02:39.695841\n",
            "[Epoch 1/10] [Batch 126/134] [D loss: 0.264587, acc:  52%] [G loss: 8.417431] time: 0:02:40.053635\n",
            "[Epoch 1/10] [Batch 127/134] [D loss: 0.262161, acc:  47%] [G loss: 8.304687] time: 0:02:40.408213\n",
            "[Epoch 1/10] [Batch 128/134] [D loss: 0.265753, acc:  57%] [G loss: 9.723262] time: 0:02:40.762109\n",
            "[Epoch 1/10] [Batch 129/134] [D loss: 0.280699, acc:  61%] [G loss: 8.923361] time: 0:02:41.114663\n",
            "[Epoch 1/10] [Batch 130/134] [D loss: 0.264626, acc:  50%] [G loss: 9.294125] time: 0:02:41.470112\n",
            "[Epoch 1/10] [Batch 131/134] [D loss: 0.275091, acc:  54%] [G loss: 9.141036] time: 0:02:41.826626\n",
            "[Epoch 1/10] [Batch 132/134] [D loss: 0.278154, acc:  61%] [G loss: 10.633053] time: 0:02:42.177389\n",
            "[Epoch 2/10] [Batch 0/134] [D loss: 0.276506, acc:  60%] [G loss: 7.149346] time: 0:02:42.539987\n",
            "Testing Image at Iteration No.0\n",
            "(1, 256, 256, 3)\n",
            "[Epoch 2/10] [Batch 1/134] [D loss: 0.278821, acc:  63%] [G loss: 10.265925] time: 0:02:43.045539\n",
            "[Epoch 2/10] [Batch 2/134] [D loss: 0.259893, acc:  47%] [G loss: 10.345731] time: 0:02:43.397923\n",
            "[Epoch 2/10] [Batch 3/134] [D loss: 0.300081, acc:  32%] [G loss: 9.869583] time: 0:02:43.757791\n",
            "[Epoch 2/10] [Batch 4/134] [D loss: 0.332924, acc:  29%] [G loss: 8.346368] time: 0:02:44.118686\n",
            "[Epoch 2/10] [Batch 5/134] [D loss: 0.280989, acc:  47%] [G loss: 9.892435] time: 0:02:44.478756\n",
            "[Epoch 2/10] [Batch 6/134] [D loss: 0.283006, acc:  33%] [G loss: 10.261787] time: 0:02:44.840864\n",
            "[Epoch 2/10] [Batch 7/134] [D loss: 0.340205, acc:  29%] [G loss: 8.268119] time: 0:02:45.194881\n",
            "[Epoch 2/10] [Batch 8/134] [D loss: 0.304453, acc:  30%] [G loss: 8.279787] time: 0:02:45.553341\n",
            "[Epoch 2/10] [Batch 9/134] [D loss: 0.264956, acc:  41%] [G loss: 13.809311] time: 0:02:45.908531\n",
            "[Epoch 2/10] [Batch 10/134] [D loss: 0.272049, acc:  58%] [G loss: 8.565660] time: 0:02:46.260217\n",
            "[Epoch 2/10] [Batch 11/134] [D loss: 0.307855, acc:  69%] [G loss: 8.635243] time: 0:02:46.625798\n",
            "[Epoch 2/10] [Batch 12/134] [D loss: 0.307853, acc:  66%] [G loss: 14.000775] time: 0:02:46.984294\n",
            "[Epoch 2/10] [Batch 13/134] [D loss: 0.288088, acc:  37%] [G loss: 9.708580] time: 0:02:47.334623\n",
            "[Epoch 2/10] [Batch 14/134] [D loss: 0.285347, acc:  46%] [G loss: 9.644746] time: 0:02:47.694392\n",
            "[Epoch 2/10] [Batch 15/134] [D loss: 0.270948, acc:  62%] [G loss: 8.453481] time: 0:02:48.047458\n",
            "[Epoch 2/10] [Batch 16/134] [D loss: 0.261635, acc:  56%] [G loss: 6.728610] time: 0:02:48.399373\n",
            "[Epoch 2/10] [Batch 17/134] [D loss: 0.280672, acc:  64%] [G loss: 8.386234] time: 0:02:48.754854\n",
            "[Epoch 2/10] [Batch 18/134] [D loss: 0.263193, acc:  57%] [G loss: 8.616964] time: 0:02:49.108441\n",
            "[Epoch 2/10] [Batch 19/134] [D loss: 0.278993, acc:  34%] [G loss: 9.532148] time: 0:02:49.460283\n",
            "[Epoch 2/10] [Batch 20/134] [D loss: 0.280030, acc:  36%] [G loss: 8.094441] time: 0:02:49.817689\n",
            "[Epoch 2/10] [Batch 21/134] [D loss: 0.269082, acc:  46%] [G loss: 10.964083] time: 0:02:50.168163\n",
            "[Epoch 2/10] [Batch 22/134] [D loss: 0.345627, acc:  70%] [G loss: 8.921704] time: 0:02:50.521681\n",
            "[Epoch 2/10] [Batch 23/134] [D loss: 0.496153, acc:  66%] [G loss: 10.802113] time: 0:02:50.890063\n",
            "[Epoch 2/10] [Batch 24/134] [D loss: 0.464850, acc:  66%] [G loss: 8.502200] time: 0:02:51.244636\n",
            "[Epoch 2/10] [Batch 25/134] [D loss: 0.347008, acc:  66%] [G loss: 8.827749] time: 0:02:51.601839\n",
            "[Epoch 2/10] [Batch 26/134] [D loss: 0.329495, acc:  64%] [G loss: 7.370049] time: 0:02:51.955430\n",
            "[Epoch 2/10] [Batch 27/134] [D loss: 0.336840, acc:  66%] [G loss: 7.129272] time: 0:02:52.309911\n",
            "[Epoch 2/10] [Batch 28/134] [D loss: 0.392154, acc:  70%] [G loss: 9.984985] time: 0:02:52.667706\n",
            "[Epoch 2/10] [Batch 29/134] [D loss: 0.529072, acc:  70%] [G loss: 9.236182] time: 0:02:53.022610\n",
            "[Epoch 2/10] [Batch 30/134] [D loss: 0.426492, acc:  69%] [G loss: 9.861521] time: 0:02:53.380662\n",
            "[Epoch 2/10] [Batch 31/134] [D loss: 0.281164, acc:  40%] [G loss: 10.223176] time: 0:02:53.737125\n",
            "[Epoch 2/10] [Batch 32/134] [D loss: 0.304763, acc:  30%] [G loss: 10.983259] time: 0:02:54.098918\n",
            "[Epoch 2/10] [Batch 33/134] [D loss: 0.298119, acc:  29%] [G loss: 7.259761] time: 0:02:54.452485\n",
            "[Epoch 2/10] [Batch 34/134] [D loss: 0.275699, acc:  41%] [G loss: 8.673870] time: 0:02:54.804350\n",
            "[Epoch 2/10] [Batch 35/134] [D loss: 0.271310, acc:  52%] [G loss: 8.162589] time: 0:02:55.161964\n",
            "[Epoch 2/10] [Batch 36/134] [D loss: 0.261710, acc:  56%] [G loss: 8.185305] time: 0:02:55.514633\n",
            "[Epoch 2/10] [Batch 37/134] [D loss: 0.282016, acc:  66%] [G loss: 7.772823] time: 0:02:55.867685\n",
            "[Epoch 2/10] [Batch 38/134] [D loss: 0.288597, acc:  67%] [G loss: 7.909347] time: 0:02:56.221985\n",
            "[Epoch 2/10] [Batch 39/134] [D loss: 0.291595, acc:  66%] [G loss: 8.125273] time: 0:02:56.573586\n",
            "[Epoch 2/10] [Batch 40/134] [D loss: 0.265558, acc:  51%] [G loss: 7.848227] time: 0:02:56.934864\n",
            "[Epoch 2/10] [Batch 41/134] [D loss: 0.260950, acc:  53%] [G loss: 7.539978] time: 0:02:57.291576\n",
            "[Epoch 2/10] [Batch 42/134] [D loss: 0.257630, acc:  46%] [G loss: 8.525096] time: 0:02:57.645229\n",
            "[Epoch 2/10] [Batch 43/134] [D loss: 0.261870, acc:  41%] [G loss: 8.214466] time: 0:02:58.007520\n",
            "[Epoch 2/10] [Batch 44/134] [D loss: 0.270686, acc:  39%] [G loss: 7.570361] time: 0:02:58.359929\n",
            "[Epoch 2/10] [Batch 45/134] [D loss: 0.269911, acc:  62%] [G loss: 8.177715] time: 0:02:58.716203\n",
            "[Epoch 2/10] [Batch 46/134] [D loss: 0.272615, acc:  63%] [G loss: 8.102444] time: 0:02:59.072959\n",
            "[Epoch 2/10] [Batch 47/134] [D loss: 0.277403, acc:  65%] [G loss: 6.367672] time: 0:02:59.423772\n",
            "[Epoch 2/10] [Batch 48/134] [D loss: 0.268347, acc:  53%] [G loss: 10.082791] time: 0:02:59.781655\n",
            "[Epoch 2/10] [Batch 49/134] [D loss: 0.267613, acc:  56%] [G loss: 8.102509] time: 0:03:00.143378\n",
            "[Epoch 2/10] [Batch 50/134] [D loss: 0.269551, acc:  53%] [G loss: 10.001867] time: 0:03:00.496893\n",
            "Testing Image at Iteration No.50\n",
            "(1, 256, 256, 3)\n",
            "[Epoch 2/10] [Batch 51/134] [D loss: 0.284883, acc:  66%] [G loss: 8.795487] time: 0:03:01.018913\n",
            "[Epoch 2/10] [Batch 52/134] [D loss: 0.278218, acc:  62%] [G loss: 9.129912] time: 0:03:01.369947\n",
            "[Epoch 2/10] [Batch 53/134] [D loss: 0.262822, acc:  60%] [G loss: 9.010343] time: 0:03:01.722534\n",
            "[Epoch 2/10] [Batch 54/134] [D loss: 0.277691, acc:  65%] [G loss: 9.082990] time: 0:03:02.085472\n",
            "[Epoch 2/10] [Batch 55/134] [D loss: 0.310660, acc:  65%] [G loss: 7.111229] time: 0:03:02.439558\n",
            "[Epoch 2/10] [Batch 56/134] [D loss: 0.304238, acc:  68%] [G loss: 8.786747] time: 0:03:02.796178\n",
            "[Epoch 2/10] [Batch 57/134] [D loss: 0.282327, acc:  59%] [G loss: 7.461535] time: 0:03:03.159290\n",
            "[Epoch 2/10] [Batch 58/134] [D loss: 0.261198, acc:  51%] [G loss: 6.839656] time: 0:03:03.514274\n",
            "[Epoch 2/10] [Batch 59/134] [D loss: 0.289355, acc:  31%] [G loss: 7.797133] time: 0:03:03.873274\n",
            "[Epoch 2/10] [Batch 60/134] [D loss: 0.300521, acc:  30%] [G loss: 8.272460] time: 0:03:04.238766\n",
            "[Epoch 2/10] [Batch 61/134] [D loss: 0.259908, acc:  49%] [G loss: 9.142959] time: 0:03:04.589525\n",
            "[Epoch 2/10] [Batch 62/134] [D loss: 0.261422, acc:  55%] [G loss: 8.171365] time: 0:03:04.943168\n",
            "[Epoch 2/10] [Batch 63/134] [D loss: 0.287665, acc:  64%] [G loss: 8.235432] time: 0:03:05.307223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-b749ebc40c95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizeOfBatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestAfter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-50-ca63231aac55>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, sizeOfBatch, testAfter)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizeOfBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mfakeIMG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mdisc_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Cx8gu6MHFIVL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Saving Model Weights for Future Use"
      ]
    },
    {
      "metadata": {
        "id": "ViVULcZgv7fT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weightsdir = '/content/drive/My Drive/ML Project/images50/'\n",
        "combined.save_weights(weightsdir+'combined_weights.h5')\n",
        "discriminator.save_weights(weightsdir+'discriminator_weights.h5')\n",
        "generator.save_weights(weightsdir+'generator_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sgPgR_c_g_yi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}